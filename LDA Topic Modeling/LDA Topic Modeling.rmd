---
title: "Supervised LDA Module"
author: "Linh Phan"
date: "2/8/2023"
output: html_document
font size: 12pt 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Lab Objective
1. Better understand Supervised LDA 
2. Learn how to use the lda package 
3. Attempt our own supervised LDA model

## Supervised Latent Dirichlet Allocation 

Most topic models, such as latent Dirichlet allocation (LDA), are unsupervised. However, 
there has been recent development to create supervised topic models, where each document is
paired with a response. 

For supervised LDA a response variable is associated with each document. The labeling of the 
response variable with each document will help to predict response variables for future unlabeled 
documents. 

In general the goal of supervised machine learning methods is to train a model so that it 
is able to properly predict unlabeled data in the future. For supervised LDA we want to train a model so that it will be able to accurately predict unlabeled data. An example would be using supervised LDA to predict how a customer will rate a restaurant based off the written review that they give that restaurant. 

In this module we will be looking at how to do make a supervised LDA model using the lda package in R. 

## Example 
To begin, we load the packages and data that we will be using. The data that we will be using is poliblog.document and poliblog.vocab which comprises a corpus of 773 political blogs. We will also use poliblog.rating which is a numeric vector of 773 which gives a liberal (-100) or conservative (100) to each docuument in the corpus. The goal of this module is to develop a supervised LDA model which will predict whether a blog is liberal or conservative based off of what is written in the blog. 

```{r,  results = 'hide', message = FALSE}
#install.packages("lda")
#install.packages("LDAvis")
#install.packages("tm")
#install.packages("servr")
library(lda)
library(LDAvis)
library(tm)
library(servr)

data("poliblog.documents")
data("poliblog.vocab")
data("poliblog.ratings")


```

After loading the data, we need to initialize the parameters, we set the number of topics for the model to 10. If using other datasets, the first step would be to preprocess the data, which means removing all of the puncuations and common stop words, however our dataset comes from the lda package already pre-process.

```{r, message = FALSE}
set.seed(123)
num.topics <- 10
params <- sample(c(-1, 1), num.topics, replace=TRUE)

```
The next step is to run the model using the slda.em function. In the code below, K represents the number of topics in the model, while vocab is a characrter specifying the vocabulary words associated with the word indices used in documents. 

```{r, message = FALSE}
result <- slda.em(documents=poliblog.documents,
                  K=num.topics,
                  vocab=poliblog.vocab,
                  num.e.iterations=10,
                  num.m.iterations=4,
                  alpha=1.0, eta=0.1,
                  poliblog.ratings / 100,
                  params,
                  variance=0.25,
                  lambda=1.0,
                  logistic=FALSE,
                  method="sLDA")

```

Once we've created our model, we can also visualize it through ggplot. The graph created below shows the topics we have as well as an estimate of how liberal or convervative each topic is. 
```{r, results = 'hide', message=FALSE}
library(ggplot2)

Topics <- apply(top.topic.words(result$topics, 5, by.score=TRUE),
                2, paste, collapse=" ")
coefs <- data.frame(coef(summary(result$model)))
theme_set(theme_bw())
coefs <- cbind(coefs, Topics=factor(Topics, Topics[order(coefs$Estimate)]))
coefs <- coefs[order(coefs$Estimate),]
qplot(Topics, Estimate, colour=Estimate, size=abs(t.value), data=coefs) +
  geom_errorbar(width=0.5, aes(ymin=Estimate-Std..Error,
                  ymax=Estimate+Std..Error)) + coord_flip()
```

An important part of our model is being able to assess whether it can accurately predict un-labled data. To do this we use the slda.predict function; the functions take a fitted sLDA model and predict the value of the response variable (or document-topic sums) for each given document.


```{r, message=FALSE}
predictions <- slda.predict(poliblog.documents,
                            result$topics, 
                            result$model,
                            alpha = 1.0,
                            eta=0.1)

qplot(predictions,
      fill=factor(poliblog.ratings),
      xlab = "predicted rating",
      ylab = "density",
      alpha=I(0.5),
      geom="density") +
  geom_vline(aes(xintercept=0)) +
  theme(legend.position = "none")

```
```{r, message=FALSE}
predicted.docsums <- slda.predict.docsums(poliblog.documents,
                                          result$topics, 
                                          alpha = 1.0,
                                          eta=0.1)

predicted.proportions <- t(predicted.docsums) / colSums(predicted.docsums)

qplot(`Topic 1`, `Topic 2`, 
      data = structure(data.frame(predicted.proportions), 
                       names = paste("Topic", 1:10)), 
      size = `Topic 3`)
```

## References: 
Chris Bail: https://github.com/cbail/textasdata 
